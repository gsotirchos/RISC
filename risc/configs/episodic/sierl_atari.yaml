name: SingleAgentRunner
kwargs:
  experiment_manager:
    name: Experiment
    kwargs:
      name: &run_name atari-dqn
      save_dir: experiment
      saving_schedule:
        name: PeriodicSchedule
        kwargs:
          off_value: false
          on_value: true
          period: 50000
  train_steps: 50000000
  # train_phase_steps: 2000  # reset-free
  max_steps_per_episode: &train_steps 27000
  test_max_steps: &test_steps 3000
  est_frequency: 250000
  test_episodes: 10
  test_random_goals: false
  seed: 0
  # stack_size: &stack_size 4
  env_fn:
    name: atari_envs
    kwargs:
      env_name: ALE/MontezumaRevenge-v5
      repeat_action_probability: 0.25  # probality = 1 / sticky_action_number
      frame_skip: 4
      screen_size: 32  # 84
      resolution: 8
      seed: 0
      # step_reward: &step_reward -1
      # goal_reward: &goal_reward 0
      # novelty_bonus: 0.15
      bonus: -10
      # render_mode: human
  agent:
    name: GCResetFree
    kwargs:
      base_agent:
        name: GoalConditionedDQNAgent
        kwargs:
          # batch_size: 128
          device: cuda
          representation_net:
            name: 'ConvNetwork'
            kwargs:
              channels: [32, 64, 64]
              kernel_sizes: [8, 4, 3]
              strides: [4, 2, 1]
              paddings: [2, 2, 1]
              mlp_layers: [512]
          optimizer_fn:
            name: 'RMSpropTF'
            kwargs:
              lr: 0.00025
              alpha: .95
              eps: 0.00001
              centered: True
          init_fn:
            name: 'xavier_uniform'
          loss_fn:
            name: 'SmoothL1Loss'
          discount_rate: &discount_rate 0.99
          reward_clip: 1
          update_period_schedule:
            name: 'PeriodicSchedule'
            kwargs:
              off_value: False
              on_value: True
              period: 4
          target_net_update_schedule:
            name: 'PeriodicSchedule'
            kwargs:
              off_value: False
              on_value: True
              period: 8000
          epsilon_schedule:
            name: 'LinearSchedule'
            kwargs:
              init_value: 1.0
              end_value: .01
              steps: 250000
          test_epsilon: .001
          # min_replay_history: 20000
          min_replay_history: -1
          log_frequency: 1000
      goal_generator:
        #name: FBGoalGenerator  # RISC
        name: OmniGoalGenerator
        kwargs:
          debug: false
          vis_frequency: 0
          weights: [1, 0, 1, 1]
          frontier_proportion: 1.0
          max_familiarity: 0.8
          temperature: 0.5
          use_success_prob: false
          use_oracle: false
      goal_switcher:
        name: TimeoutGoalSwitcher
        kwargs:
          switching_probability: 0.2
          use_oracle: false
      replay_buffer:
        # name: CountsReplayBuffer
        name: HERReplayBuffer  # HER
        kwargs:
          capacity: 1000000
          gamma: 0.5
          her_ratio: 0.5  # HER
          # stack_size: *stack_size
      separate_agents: false
      local_visitation_vis_frequency: 2000
      directions: [forward, teleport_backward, lateral]
      phase_step_limit: [25000, 1, 2000]
  loggers:
    - name: WandbLogger
      kwargs:
        name: *run_name
        project: Debug
        resume: allow
        start_method: fork
